/*
Copyright Â© 2023 - 2024 SUSE LLC

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package p1_test

import (
	"fmt"
	"maps"
	"strconv"
	"strings"
	"testing"
	"time"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
	"github.com/rancher-sandbox/ele-testhelpers/tools"
	. "github.com/rancher-sandbox/qase-ginkgo"
	"github.com/rancher/shepherd/clients/rancher"
	management "github.com/rancher/shepherd/clients/rancher/generated/management/v3"
	"github.com/rancher/shepherd/extensions/clusters"
	"github.com/rancher/shepherd/extensions/clusters/eks"
	"github.com/rancher/shepherd/pkg/config"
	namegen "github.com/rancher/shepherd/pkg/namegenerator"
	"k8s.io/utils/pointer"

	"github.com/rancher/hosted-providers-e2e/hosted/eks/helper"
	"github.com/rancher/hosted-providers-e2e/hosted/helpers"
)

var (
	ctx         helpers.RancherContext
	clusterName string
	testCaseID  int64
	region      = helpers.GetEKSRegion()
)

func TestP1(t *testing.T) {
	RegisterFailHandler(Fail)
	RunSpecs(t, "P1 Suite")
}

var _ = SynchronizedBeforeSuite(func() []byte {
	helpers.CommonSynchronizedBeforeSuite()
	return nil
}, func() {
	ctx = helpers.CommonBeforeSuite()
})

var _ = BeforeEach(func() {
	clusterName = namegen.AppendRandomString(helpers.ClusterNamePrefix)
})

var _ = ReportBeforeEach(func(report SpecReport) {
	// Reset case ID
	testCaseID = -1
})

var _ = ReportAfterEach(func(report SpecReport) {
	// Add result in Qase if asked
	Qase(testCaseID, report)
})

// updateClusterInUpdatingState runs checks to ensure cluster in an updating state can be updated
func updateClusterInUpdatingState(cluster *management.Cluster, client *rancher.Client, upgradeToVersion string) {
	var (
		exists bool
		err    error
	)
	cluster, err = helper.UpgradeClusterKubernetesVersion(cluster, upgradeToVersion, client, false)
	Expect(err).To(BeNil())
	Expect(*cluster.EKSConfig.KubernetesVersion).To(Equal(upgradeToVersion))

	err = clusters.WaitClusterToBeInUpgrade(client, cluster.ID)
	Expect(err).To(BeNil())

	loggingTypes := []string{"api"}
	cluster, err = helper.UpdateLogging(cluster, client, loggingTypes, false)
	Expect(err).To(BeNil())
	Expect(*cluster.EKSConfig.LoggingTypes).Should(HaveExactElements(loggingTypes))

	err = clusters.WaitClusterToBeUpgraded(client, cluster.ID)
	Expect(err).To(BeNil())

	Eventually(func() bool {
		GinkgoLogr.Info("Waiting for the updated changes to appear in EKSStatus.UpstreamSpec ...")
		cluster, err = client.Management.Cluster.ByID(cluster.ID)
		Expect(err).To(BeNil())

		for _, loggingType := range loggingTypes {
			exists = helpers.ContainsString(*cluster.EKSStatus.UpstreamSpec.LoggingTypes, loggingType)
		}
		return exists && *cluster.EKSStatus.UpstreamSpec.KubernetesVersion == upgradeToVersion
	}, "15m", "30s").Should(BeTrue())
}

func syncK8sVersionUpgradeCheck(cluster *management.Cluster, client *rancher.Client, upgradeNodeGroup bool, k8sVersion, upgradeToVersion string) {
	var err error
	GinkgoLogr.Info("Upgrading cluster to version:" + upgradeToVersion)

	By("upgrading control plane", func() {
		err = helper.UpgradeEKSClusterOnAWS(region, clusterName, upgradeToVersion)
		Expect(err).To(BeNil())

		Eventually(func() string {
			GinkgoLogr.Info("Waiting for k8s upgrade to appear in EKSStatus.UpstreamSpec ...")
			cluster, err = client.Management.Cluster.ByID(cluster.ID)
			Expect(err).To(BeNil())
			return *cluster.EKSStatus.UpstreamSpec.KubernetesVersion
		}, tools.SetTimeout(5*time.Minute), 10*time.Second).Should(Equal(upgradeToVersion), "Failed while waiting for k8s upgrade to appear in EKSStatus.UpstreamSpec")

		if !helpers.IsImport {
			// For imported clusters, EKSConfig always has null values; so we check EKSConfig only when testing provisioned clusters
			for _, ng := range cluster.EKSConfig.NodeGroups {
				Expect(*ng.Version).To(BeEquivalentTo(k8sVersion), "EKSConfig.NodePools check failed")
			}
		}
	})

	if upgradeNodeGroup {
		By("upgrading the nodegroup", func() {
			GinkgoLogr.Info(fmt.Sprintf("Upgrading Nodegroup's EKS version to %s", upgradeToVersion))
			for _, ng := range cluster.EKSStatus.UpstreamSpec.NodeGroups {
				err = helper.UpgradeEKSNodegroupOnAWS(region, clusterName, *ng.NodegroupName, upgradeToVersion)
				Expect(err).To(BeNil())
			}

			Eventually(func() bool {
				GinkgoLogr.Info("Waiting for the nodegroup upgrade to appear in EKSStatus.UpstreamSpec ...")
				cluster, err = client.Management.Cluster.ByID(cluster.ID)
				Expect(err).To(BeNil())
				for _, ng := range cluster.EKSStatus.UpstreamSpec.NodeGroups {
					if ng.Version == nil || *ng.Version != upgradeToVersion {
						return false
					}
				}
				return true
			}, tools.SetTimeout(7*time.Minute), 10*time.Second).Should(BeTrue(), "Failed while waiting for nodegroup k8s upgrade to appear in EKSStatus.UpstreamSpec")

			if !helpers.IsImport {
				// For imported clusters, EKSConfig always has null values; so we check EKSConfig only when testing provisioned clusters
				Expect(*cluster.EKSConfig.KubernetesVersion).To(Equal(upgradeToVersion))
				for _, ng := range cluster.EKSConfig.NodeGroups {
					Expect(ng.Version).ToNot(BeNil())
					Expect(*ng.Version).To(BeEquivalentTo(upgradeToVersion), "EKSConfig.NodePools upgrade check failed")
				}
			}
		})
	}
}

func syncAWSToRancherCheck(cluster *management.Cluster, client *rancher.Client, k8sVersion, upgradeToVersion string) {
	loggingTypes := []string{"api", "audit", "authenticator", "controllerManager", "scheduler"}
	By("Enabling the LoggingTypes", func() {
		err := helper.UpdateLoggingOnAWS(clusterName, region, loggingTypes, nil)
		Expect(err).To(BeNil())

		Eventually(func() bool {
			cluster, err = client.Management.Cluster.ByID(cluster.ID)
			Expect(err).To(BeNil())
			updated := len(*cluster.EKSStatus.UpstreamSpec.LoggingTypes) == len(loggingTypes)
			if !helpers.IsImport {
				updated = updated && len(*cluster.EKSConfig.LoggingTypes) == len(loggingTypes)
			}
			return updated
		}, "10m", "7s").Should(BeTrue(), "Timed out waiting for LoggingTypes update to appear in Rancher")

		for _, lType := range loggingTypes {
			if !helpers.IsImport {
				Expect(*cluster.EKSConfig.LoggingTypes).To(ContainElement(lType))
			}
			Expect(*cluster.EKSStatus.UpstreamSpec.LoggingTypes).To(ContainElement(lType))
		}
	})

	By("Disabling the LoggingTypes", func() {
		err := helper.UpdateLoggingOnAWS(clusterName, region, nil, []string{"all"})
		Expect(err).To(BeNil())
		Eventually(func() bool {
			cluster, err = client.Management.Cluster.ByID(cluster.ID)
			Expect(err).To(BeNil())
			updated := len(*cluster.EKSStatus.UpstreamSpec.LoggingTypes) == 0
			if !helpers.IsImport {
				updated = updated && len(*cluster.EKSConfig.LoggingTypes) == 0
			}
			return updated
		}, "10m", "7s").Should(BeTrue(), "Timed out waiting for LoggingTypes update to appear in Rancher")
	})

	By("Updating public/private access and CIDRs", func() {
		cidrs := []string{"0.0.0.0/0", helpers.GetRancherIP() + "/32"}
		err := helper.UpdateVPCAccess(clusterName, region, true, true, cidrs)
		Expect(err).To(BeNil())
		Eventually(func() bool {
			cluster, err = client.Management.Cluster.ByID(cluster.ID)
			Expect(err).To(BeNil())
			privateUpdated := *cluster.EKSStatus.UpstreamSpec.PrivateAccess
			publicUpdated := *cluster.EKSStatus.UpstreamSpec.PublicAccess
			cidrUpdated := len(*cluster.EKSStatus.UpstreamSpec.PublicAccessSources) == len(cidrs)
			if !helpers.IsImport {
				privateUpdated = privateUpdated && *cluster.EKSConfig.PrivateAccess
				publicUpdated = publicUpdated && *cluster.EKSConfig.PublicAccess
				cidrUpdated = cidrUpdated && len(*cluster.EKSConfig.PublicAccessSources) == len(cidrs)
			}
			return privateUpdated && publicUpdated && cidrUpdated
		}, "10m", "7s").Should(BeTrue(), "Timed out waiting for private/public access to appear in Rancher")

		for _, cidr := range cidrs {
			if !helpers.IsImport {
				Expect(*cluster.EKSConfig.PublicAccessSources).To(ContainElement(cidr))
			}
			Expect(*cluster.EKSStatus.UpstreamSpec.PublicAccessSources).To(ContainElement(cidr))
		}
	})

	By("upgrading control plane and nodegroup", func() {
		syncK8sVersionUpgradeCheck(cluster, client, true, k8sVersion, upgradeToVersion)
	})

	By("scaling up the NodeGroup", func() {
		ng := cluster.EKSStatus.UpstreamSpec.NodeGroups[0]
		var nodeCount int64 = 2
		if ng.DesiredSize != nil {
			nodeCount = *ng.DesiredSize + 2
		}
		err := helper.ScaleNodeGroupOnAWS(*ng.NodegroupName, clusterName, region, nodeCount, nodeCount+2, nodeCount-1)
		Expect(err).To(BeNil())
		Eventually(func() bool {
			cluster, err = client.Management.Cluster.ByID(cluster.ID)
			Expect(err).To(BeNil())
			updated := *cluster.EKSStatus.UpstreamSpec.NodeGroups[0].DesiredSize == nodeCount
			if !helpers.IsImport {
				updated = updated && *cluster.EKSConfig.NodeGroups[0].DesiredSize == nodeCount
			}
			return updated
		}, "10m", "7s").Should(BeTrue(), "Timed out waiting for NodeGroup scale to show in Rancher")
	})

	var nodeName = namegen.AppendRandomString("ng")
	ngCount := len(cluster.EKSStatus.UpstreamSpec.NodeGroups)
	if helpers.IsImport {
		// The following error is encountered when adding nodegroup to a non-eksctl managed i.e. rancher provisioned cluster; so we skip it for now
		// Error: loading VPC spec for cluster "auto-eks-hp-ci-atiia": VPC configuration required for creating nodegroups on clusters not owned by eksctl: vpc.subnets, vpc.id, vpc.securityGroup
		// If this is implemented for rancher-provisioned cluster; make sure to check for Config spec.
		By("adding a NodeGroup", func() {
			err := helper.AddNodeGroupOnAWS(nodeName, clusterName, region)
			Expect(err).To(BeNil())

			Eventually(func() bool {
				cluster, err = client.Management.Cluster.ByID(cluster.ID)
				Expect(err).To(BeNil())
				if len(cluster.EKSStatus.UpstreamSpec.NodeGroups) != ngCount+1 {
					return false
				}
				for _, ng := range cluster.EKSStatus.UpstreamSpec.NodeGroups {
					if *ng.NodegroupName == nodeName {
						return true
					}
				}
				return false
			}, "10m", "7s").Should(BeTrue(), "Timed out waiting for new NodeGroup to appear in Rancher")
		})
	}

	By("deleting a NodeGroup", func() {
		if !helpers.IsImport {
			// adding an extra nodegroup to make sure there are at least 2 nodes in the cluster before deleting it for rancher-provisioned clusters
			// remove this if and when By("adding a Nodegroup") is implemented for rancher-provisioned cluster
			var err error
			cluster, err = helper.AddNodeGroup(cluster, 1, client, true, true)
			Expect(err).To(BeNil())
			nodeName = *cluster.EKSConfig.NodeGroups[1].NodegroupName
		}
		err := helper.ModifyEKSNodegroupOnAWS(region, clusterName, nodeName, "delete", "--wait")
		Expect(err).To(BeNil())
		Eventually(func() bool {
			cluster, err = client.Management.Cluster.ByID(cluster.ID)
			Expect(err).To(BeNil())
			updated := len(cluster.EKSStatus.UpstreamSpec.NodeGroups) != ngCount
			if !helpers.IsImport {
				updated = updated && len(cluster.EKSConfig.NodeGroups) != ngCount
			}
			if updated {
				return false
			}
			var nodeGroupPresentInUpstream, nodeGroupPresentInConfig bool
			for _, ng := range cluster.EKSStatus.UpstreamSpec.NodeGroups {
				if *ng.NodegroupName == nodeName {
					nodeGroupPresentInUpstream = true
					break
				}
			}
			if !helpers.IsImport {
				for _, ng := range cluster.EKSConfig.NodeGroups {
					if *ng.NodegroupName == nodeName {
						nodeGroupPresentInConfig = true
						break
					}
				}
			} else {
				// if the cluster is imported, Config will be null, so we assign this variable to true to do easy check
				nodeGroupPresentInConfig = true
			}
			return nodeGroupPresentInConfig && nodeGroupPresentInUpstream
		}, "10m", "7s").Should(BeFalse(), "Timed out waiting for NodeGroup to delete from Rancher")
	})

	tags := map[string]string{"foo": "bar", "updated": "via-cli"}
	By("adding tags to EKS cluster", func() {
		err := helper.AddClusterTagsOnAWS(clusterName, region, tags)
		Expect(err).To(BeNil())
		Eventually(func() bool {
			cluster, err = client.Management.Cluster.ByID(cluster.ID)
			Expect(err).To(BeNil())
			upstreamTags := *cluster.EKSStatus.UpstreamSpec.Tags
			for key := range tags {
				_, existsUpstream := upstreamTags[key]

				var existsConfig bool
				if !helpers.IsImport {
					configTags := *cluster.EKSConfig.Tags
					_, existsConfig = configTags[key]
				} else {
					// if the cluster is imported, Config will be null, so we assign this variable to true to do easy check
					existsConfig = true
				}
				if !(existsConfig && existsUpstream) {
					// TODO: check this logic
					// return early if the key doesn't exist in either of the specs
					return false
				}
			}
			return true
		}, "10m", "5s").Should(BeTrue(), "Timed out waiting for EKS tags to be updated")
		for key, value := range tags {
			if !helpers.IsImport {
				Expect(*cluster.EKSConfig.Tags).To(HaveKeyWithValue(key, value))
			}
			Expect(*cluster.EKSStatus.UpstreamSpec.Tags).To(HaveKeyWithValue(key, value))
		}
	})

	By("removing tags from EKS cluster", func() {
		var removeTags []string
		for key := range tags {
			removeTags = append(removeTags, key)
		}
		err := helper.RemoveClusterTagsOnAWS(clusterName, region, removeTags)
		Expect(err).To(BeNil())
		Eventually(func() bool {
			cluster, err = client.Management.Cluster.ByID(cluster.ID)
			Expect(err).To(BeNil())
			upstreamTags := *cluster.EKSStatus.UpstreamSpec.Tags
			for key := range tags {
				_, existsUpstream := upstreamTags[key]
				var existsConfig bool
				if !helpers.IsImport {
					configTags := *cluster.EKSConfig.Tags
					_, existsConfig = configTags[key]
				}
				if existsConfig || existsUpstream {
					return false
				}
			}
			return true
		}, "10m", "5s").Should(BeTrue(), "Timed out waiting for EKS tags to be removed")
	})

	addLabels := map[string]string{"foo": "bar", "updated": "via-cli"}
	const ngIndex = 0
	By("add labels to Nodegroup", func() {
		err := helper.UpdateNodeGroupLabelsOnAWS(clusterName, *cluster.EKSStatus.UpstreamSpec.NodeGroups[ngIndex].NodegroupName, region, addLabels, nil)
		Expect(err).To(BeNil())
		Eventually(func() bool {
			cluster, err = client.Management.Cluster.ByID(cluster.ID)
			Expect(err).To(BeNil())
			upstreamLabels := *cluster.EKSStatus.UpstreamSpec.NodeGroups[ngIndex].Labels

			for key := range addLabels {
				_, existUpstream := upstreamLabels[key]
				var existConfig bool
				if !helpers.IsImport {
					configTags := *cluster.EKSConfig.NodeGroups[ngIndex].Labels
					_, existConfig = configTags[key]
				} else {
					// if the cluster is imported, Config will be null, so we assign this variable to true to do easy check
					existConfig = true
				}
				if !(existConfig && existUpstream) {
					// return early if the key doesn't exist in either of the specs
					return false
				}
			}
			return true
		}, "10m", "5s").Should(BeTrue(), "Timed out waiting for EKS nodegroup labels to be added")

		for key, value := range addLabels {
			if !helpers.IsImport {
				Expect(*cluster.EKSConfig.NodeGroups[ngIndex].Labels).To(HaveKeyWithValue(key, value))
			}
			Expect(*cluster.EKSStatus.UpstreamSpec.NodeGroups[ngIndex].Labels).To(HaveKeyWithValue(key, value))
		}
	})

	By("deleting labels from Nodegroup", func() {
		var removeLabels []string
		for key := range addLabels {
			removeLabels = append(removeLabels, key)
		}
		err := helper.UpdateNodeGroupLabelsOnAWS(clusterName, *cluster.EKSStatus.UpstreamSpec.NodeGroups[ngIndex].NodegroupName, region, nil, removeLabels)
		Expect(err).To(BeNil())
		Eventually(func() bool {
			cluster, err = client.Management.Cluster.ByID(cluster.ID)
			Expect(err).To(BeNil())
			upstreamLabels := *cluster.EKSStatus.UpstreamSpec.NodeGroups[ngIndex].Labels
			for _, key := range removeLabels {
				_, existsUpstream := upstreamLabels[key]
				var existsConfig bool
				if !helpers.IsImport {
					configTags := *cluster.EKSConfig.NodeGroups[ngIndex].Labels
					_, existsConfig = configTags[key]
				}
				if existsConfig || existsUpstream {
					// return early if the key exists in either of the specs
					return false
				}
			}
			return true
		}, "10m", "5s").Should(BeTrue(), "Timed out waiting for EKS nodegroup labels to be deleted")

		for key, value := range addLabels {
			if !helpers.IsImport {
				Expect(*cluster.EKSConfig.NodeGroups[ngIndex].Labels).ToNot(HaveKeyWithValue(key, value))
			}
			Expect(*cluster.EKSStatus.UpstreamSpec.NodeGroups[ngIndex].Labels).ToNot(HaveKeyWithValue(key, value))
		}
	})
}

func syncRancherToAWSCheck(cluster *management.Cluster, client *rancher.Client, k8sVersion, upgradeToVersion string) {
	var err error
	loggingTypes := []string{"api", "audit", "authenticator", "controllerManager", "scheduler"}
	currentNodeGroupNumber := len(cluster.EKSConfig.NodeGroups)
	initialNodeCount := *cluster.EKSConfig.NodeGroups[0].DesiredSize

	By("upgrading control plane", func() {
		syncK8sVersionUpgradeCheck(cluster, client, false, k8sVersion, upgradeToVersion)
	})

	By("scaling up the NodeGroup", func() {
		cluster, err = helper.ScaleNodeGroup(cluster, client, initialNodeCount+1, true, true)
		Expect(err).To(BeNil())

		// Verify the existing details do NOT change in Rancher
		for _, ng := range cluster.EKSConfig.NodeGroups {
			Expect(*ng.Version).To(BeEquivalentTo(k8sVersion))
			Expect(len(cluster.EKSConfig.NodeGroups)).Should(BeNumerically("==", currentNodeGroupNumber))
		}

		// Verify the new edits reflect in AWS and existing details do NOT change
		var out string
		out, err = helper.GetFromEKS(region, clusterName, "cluster", "'.[]|.Version'")
		Expect(err).To(BeNil())
		Expect(out).To(Equal(upgradeToVersion))

		out, err = helper.GetFromEKS(region, clusterName, "nodegroup", "'.|length'")
		Expect(err).To(BeNil())
		Expect(strconv.Atoi(out)).To(Equal(currentNodeGroupNumber))

		out, err = helper.GetFromEKS(region, clusterName, "nodegroup", "'.[]|.DesiredCapacity'")
		Expect(err).To(BeNil())
		Expect(strconv.ParseInt(out, 10, 64)).To(Equal(initialNodeCount + 1))
	})

	By("adding a NodeGroup", func() {
		cluster, err = helper.AddNodeGroup(cluster, 1, client, true, true)
		Expect(err).To(BeNil())

		// Verify the existing details do NOT change in Rancher
		Expect(*cluster.EKSConfig.KubernetesVersion).To(Equal(upgradeToVersion))
		Expect(*cluster.EKSConfig.LoggingTypes).ShouldNot(HaveExactElements(loggingTypes))

		// Verify the new edits reflect in AWS console and existing details do NOT change
		var out string
		out, err = helper.GetFromEKS(region, clusterName, "cluster", "'.[]|.Version'")
		Expect(err).To(BeNil())
		Expect(out).To(Equal(upgradeToVersion))

		out, err = helper.GetFromEKS(region, clusterName, "nodegroup", "'.|length'")
		Expect(err).To(BeNil())
		Expect(strconv.Atoi(out)).To(Equal(currentNodeGroupNumber + 1))
	})

	By("Adding the LoggingTypes", func() {
		cluster, err = helper.UpdateLogging(cluster, client, loggingTypes, true)
		Expect(err).To(BeNil())

		// Verify the existing details do NOT change in Rancher
		Expect(*cluster.EKSConfig.KubernetesVersion).To(Equal(upgradeToVersion))
		Expect(len(cluster.EKSConfig.NodeGroups)).To(Equal(currentNodeGroupNumber + 1))

		// Verify the new edits reflect in AWS console and existing details do NOT change
		var out string
		out, err = helper.GetFromEKS(region, clusterName, "nodegroup", "'.|length'")
		Expect(err).To(BeNil())
		Expect(strconv.Atoi(out)).To(Equal(currentNodeGroupNumber + 1))

		out, err = helper.GetFromEKS(region, clusterName, "cluster", "'.[]|.Logging|.[]|.[]|.Types'")
		Expect(err).To(BeNil())
		Expect(out).ShouldNot(HaveExactElements(loggingTypes))
	})

}

// upgradeNodeKubernetesVersionGTCP upgrades Nodegroup version greater than Controlplane's
func upgradeNodeKubernetesVersionGTCPCheck(cluster *management.Cluster, client *rancher.Client, upgradeToVersion string) {
	GinkgoLogr.Info("Upgrading only Nodegroup's EKS version to: " + upgradeToVersion)
	var err error
	cluster, err = helper.UpgradeNodeKubernetesVersion(cluster, upgradeToVersion, client, false, false)
	Expect(err).To(BeNil())

	// wait until the error is visible on the cluster
	Eventually(func() bool {
		cluster, err = client.Management.Cluster.ByID(cluster.ID)
		Expect(err).To(BeNil())
		// checking for both the messages since different operator version shows different messages. To be removed once the message is updated.
		// New message:  versions for cluster [1.29] and nodegroup [1.30] not compatible: all nodegroup kubernetes versions must be equal to or one minor version lower than the cluster kubernetes version
		return cluster.Transitioning == "error" && strings.Contains(cluster.TransitioningMessage, "not compatible")
	}, "1m", "3s").Should(BeTrue())
}

// invalidEndpointCheck updates PublicAccess Sources
func invalidEndpointCheck(cluster *management.Cluster, client *rancher.Client) {
	var err error
	cidr := []string{namegen.AppendRandomString("invalid")}
	cluster, _ = helper.UpdatePublicAccessSources(cluster, client, cidr, false)

	Eventually(func() bool {
		cluster, err = client.Management.Cluster.ByID(cluster.ID)
		Expect(err).To(BeNil())
		return cluster.Transitioning == "error" && strings.Contains(cluster.TransitioningMessage, "InvalidParameterException: The following CIDRs are invalid in publicAccessCidrs")
	}, "2m", "3s").Should(BeTrue())
}

// invalidAccessCheck disbales both PublicAccess & PrivateAccess
func invalidAccessValuesCheck(cluster *management.Cluster, client *rancher.Client) {
	var err error
	_, err = helper.UpdateAccess(cluster, client, false, false, false)
	Expect(err).To(MatchError(ContainSubstring("public access, private access, or both must be enabled")))
}

func upgradeCPAndAddNgCheck(cluster *management.Cluster, client *rancher.Client, upgradeToVersion string) {
	var err error
	originalLen := len(cluster.EKSConfig.NodeGroups)
	newNodeGroupName := pointer.String(namegen.AppendRandomString("ng"))
	GinkgoLogr.Info("Upgrading control plane to version:" + upgradeToVersion)

	By("upgrading the ControlPlane", func() {
		cluster, err = helper.UpgradeClusterKubernetesVersion(cluster, upgradeToVersion, ctx.RancherAdminClient, true)
		Expect(err).To(BeNil())
	})

	var eksClusterConfig management.EKSClusterConfigSpec
	config.LoadConfig(eks.EKSClusterConfigConfigurationFileKey, &eksClusterConfig)

	updateFunc := func(cluster *management.Cluster) {
		var updatedNodeGroupsList []management.NodeGroup
		newNodeGroup := eksClusterConfig.NodeGroups[0]
		newNodeGroup.NodegroupName = newNodeGroupName
		updatedNodeGroupsList = append(updatedNodeGroupsList, newNodeGroup)
		cluster.EKSConfig.NodeGroups = updatedNodeGroupsList
	}

	cluster, err = helper.UpdateCluster(cluster, client, updateFunc)
	Expect(err).To(BeNil())
	Expect(len(cluster.EKSConfig.NodeGroups)).To(BeEquivalentTo(originalLen))
	for _, ng := range cluster.EKSConfig.NodeGroups {
		Expect(ng.NodegroupName).To(Equal(newNodeGroupName))
	}

	err = clusters.WaitClusterToBeUpgraded(client, cluster.ID)
	Expect(err).To(BeNil())

	// wait until the update is visible on the cluster
	Eventually(func() bool {
		GinkgoLogr.Info("Waiting for the version of new nodegroup to appear in EKSStatus.UpstreamSpec ...")
		cluster, err = ctx.RancherAdminClient.Management.Cluster.ByID(cluster.ID)
		Expect(err).To(BeNil())
		for _, ng := range cluster.EKSStatus.UpstreamSpec.NodeGroups {
			if ng.Version == nil || *ng.Version != upgradeToVersion {
				return false
			}
		}
		return true
	}, "5m", "15s").Should(BeTrue())
}

// Automate Qase 81 and 131
func updateTagsAndLabels(cluster *management.Cluster, client *rancher.Client) {
	var err error
	tags := map[string]string{
		"foo":        "bar",
		"testCaseID": "144-97-143",
	}

	labels := map[string]string{
		"testCaseID": "142-99-145",
	}

	originalClusterTags := *cluster.EKSConfig.Tags
	// updatedTags must contain both the original and the new tags
	updatedTags := make(map[string]string)
	maps.Copy(updatedTags, originalClusterTags)
	maps.Copy(updatedTags, tags)

	By("Adding cluster tags", func() {
		cluster, err = helper.UpdateClusterTags(cluster, client, updatedTags, true)
		Expect(err).To(BeNil())
	})

	By("Removing cluster tags", func() {
		cluster, err = helper.UpdateClusterTags(cluster, client, originalClusterTags, true)
		for key, value := range tags {
			Expect(*cluster.EKSConfig.Tags).ToNot(HaveKeyWithValue(key, value))
		}
	})

	originalNGLabels := *cluster.EKSConfig.NodeGroups[0].Labels
	// updatedNGLabels must contain both the original and the new tags
	updatedNGLabels := make(map[string]string)
	maps.Copy(updatedNGLabels, originalNGLabels)
	maps.Copy(updatedNGLabels, labels)

	originalNGTags := *cluster.EKSConfig.NodeGroups[0].Tags
	// updatedNGTags must contain both the original and the new tags
	updatedNGTags := make(map[string]string)
	maps.Copy(updatedNGTags, originalNGTags)
	maps.Copy(updatedNGTags, tags)

	By("Adding Nodegroup tags & labels", func() {
		cluster, err = helper.UpdateNodegroupMetadata(cluster, client, updatedNGTags, updatedNGLabels, true)
		Expect(err).To(BeNil())
	})

	By("Removing Nodegroup tags & labels", func() {
		cluster, err = helper.UpdateNodegroupMetadata(cluster, client, originalNGTags, originalNGLabels, true)
		for _, ng := range cluster.EKSConfig.NodeGroups {
			for key, value := range tags {
				Expect(*ng.Tags).ToNot(HaveKeyWithValue(key, value))
			}
			for key, value := range labels {
				Expect(*ng.Labels).ToNot(HaveKeyWithValue(key, value))
			}
		}
	})
}

// Automates Qase: 128 and 77
func updateLoggingCheck(cluster *management.Cluster, client *rancher.Client) {
	var err error
	loggingTypes := []string{"api", "audit", "authenticator", "controllerManager", "scheduler"}
	By("Adding the LoggingTypes", func() {
		cluster, err = helper.UpdateLogging(cluster, client, loggingTypes, true)
		Expect(err).To(BeNil())
	})

	By("Removing the LoggingTypes", func() {
		cluster, err = helper.UpdateLogging(cluster, client, []string{loggingTypes[0]}, true)
		Expect(err).To(BeNil())
	})
}
